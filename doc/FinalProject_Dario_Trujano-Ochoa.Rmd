---
title: "My UCSB Machine Learning Project on Community Notes"
author: "Dario Trujano-Ochoa"
date: "Fall 2023"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Embarking on my Machine Learning (ML) project I am delving into the world of Community Notes on X, formerly known as Twitter. 
I will use ML tools to find the best model to predict the number of ratings that each notes has received.

![Fig 1. Community Notes Logo^[By Community Notes - https://twitter.com/CommunityNotes/photo, Public Domain, https://commons.wikimedia.org/w/index.php?curid=141534850]](images/Community_Notes_logo.png){ width=30%, style="display: block; margin: 0 auto;" }

## What are Community Notes?

Community Notes stands out as an innovative platform where contributors collaboratively add context to potentially misleading posts, challenging conventional content moderation methods. The publication of a Community Note is driven not by a majority rule but by the agreement of contributors who have previously disagreed, creating a transparent, community-driven approach to combat misinformation.
This sounds like a great idea, but it has been proven sometimes good but insufficient^[https://www.lemonde.fr/en/pixels/article/2023/07/03/i-spent-one-week-as-an-arbiter-of-truth-on-twitter-s-community-notes-service_6042188_13.html] or irrelevant^[https://mashable.com/article/twitter-x-community-notes-misinformation-views-investigation], and even susceptible to disinformation^[https://www.wired.com/story/x-community-notes-failures/], as you can see in more detail in the Wikipedia page dedicated to [Community Notes](https://en.wikipedia.org/wiki/Community_Notes#Criticisms_and_analysis) (CN).

![Fig 2. Community Notes Logo^[By Community Notes - https://github.com/twitter/communitynotes]](images/help-rate-this-note-expanded.png){ width=30%, style="display: block; margin: 0 auto;" }

## How are Notes posted?

At the core of this exploration is the open-source algorithm powering Community Notes, described as ["insanely complicated."](https://uk.finance.yahoo.com/news/bird-watching-going-x-twitter-111442959.html?guccounter=1&guce_referrer=aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnLw&guce_referrer_sig=AQAAAJdoz-CTbAiZoR9yDHum0zUXTqVwhTXSB93ig32XLjzMO5rwCrk9QPJbcaqLCfwL2PcxqZgwd9zWLatoZAJkejGVwDHysvbYkRqcylDIefkFpoRgHDr1O4pvWFXlDV1Dfox1QJGZbGqJsszkp4VCvs_OOEGJVG6QqtL1QyDNA2tZ) This algorithm ensures that notes are rated by a diverse range of perspectives, incorporating an opinion classification based on contributors' alignment with the left and right-wing political spectrum. 
It is only after people that previusly disagree, agree on the helpfulness of a note that the note is posted.
Therefore, the number of ratings that a note receives is very important to determine if the note is ever published, and how fast.

![Fig 3. Community Notes Rating)^[By Twitter - Original publication: Screenshot from CommunityNotesContributorImmediate source: https://twitter.com/i/communitynotes, Fair use, https://en.wikipedia.org/w/index.php?curid=75348629]](images/CommunityNotesRating.png){width=50%, style="display: block; margin: 0 auto;"}

## What this project explore?

The project is centered around a vast dataset comprising over 300,000 notes, each representing a collaborative effort to combat misinformation. Of particular significance is the attempt to predict the number of ratings received by each note, as this is a crucial determinant in deciding whether a note is published. This predictive aspect adds a layer of complexity to our analysis, aiming to uncover insights into the collaborative evaluation system and its impact on the publication of notes.

This notes can be related to any topic and even advertising. 
It is worth noting that the most rated notes was about a [game](https://twitter.com/Evony_TKR/status/1672908357081124864).

The openness of the data invite scrutiny and analysis, fostering an environment where skepticism can be transformed into informed inquiry. Join me on this journey as we explore the intricacies of Community Notes.

# Data Set

The data from the notes and the ratings are open to anyone with an account on X. On the github page of CN you can also find the code and algorithm. 
Here are the sources:

- Data from the project from X, Community Notes, can be found [here](https://twitter.com/i/communitynotes/download-data).

  - The explanation of the data can be found [here](https://communitynotes.twitter.com/guide/en/under-the-hood/download-data).

- And the code from Community Notes is in [github](https://github.com/twitter/communitynotes).

Since the data sets are very large, I save the final data set with the information I needed from each one. 
In this section I explain the original data sets and the creation of the final merged data used for the present project. 

## Original Files

### Notes

### Ratings



## Original Open Source Data

## Data Preparation

```{r Load Packages}
# Load Packages
library(pacman)
p_load(tidyverse, 
       tidymodels, 
       recipes,
       kknn,
       yardstick,
       tune,
       ggplot2,
       ggthemes,
       rsample,
       parsnip,
       workflows
       )
```

# EDA

## Final Data

```{r}
load("../data/notes_merged.RData")
```


## Relationship between Ratings and Classification 

# Prepare Data for Analysis 

```{r}
# To reproduce the results
set.seed(1984)

# Percentage used for the training set
training_percentage <- 0.75

# Splitting the data
split_notes <- initial_split(notes_merged,prop = training_percentage)
training_notes <- training(split_notes)
test_notes <- testing(split_notes)
```

# Model Building

```{r}
# recipe ----
rec_reg <- recipe(ratings ~  ., 
              data = training_notes %>% select(-c(note_id, tweet_id ,summary))) %>% 
  step_normalize(agreement_rate) 


# models ----
##linear model ----
linear_reg <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

# KNN model
k = 7
knn_mod <- nearest_neighbor(neighbors = k) %>%
  set_mode("regression") %>%
  set_engine("kknn")

# Workflow ----
## linear ----
lm_wkflow <- workflow() %>% 
  # add model
  add_model(linear_reg) %>% 
  # add receipe
  add_recipe(rec_reg)

## KNNn model ----
knn_wkflow <- workflow() %>% 
  # add model
  add_model(knn_mod) %>% 
  # add receipe
  add_recipe(rec_reg)

```

# Model Fitting

```{r}
# fitting models ----
## linear ----
# fit_lm <- 
#   lm_wkflow %>% 
#   fit(data = training_notes)
# fit_lm
# 
# ## KNN ----
# fit_knn <- 
#   knn_wkflow %>% 
#   fit(data = training_notes)
# fit_knn
# 
# # metrics
# notes_metrics <- metric_set(rmse, rsq, mae)
# 
# # linear model
# notes_lm_aug <- augment(fit_lm, test_notes)
# notes_metrics(notes_lm_aug, truth = ratings,
#                 estimate = .pred)
# 
# notes_merged %>% ggplot() +
#   geom_point(aes(x=agreement_rate,y=ratings))

# knn model
# notes_knn_aug <- augment(fit_knn, test_notes)
# notes_metrics(notes_knn_aug, truth = ratings,
#               estimate = .pred)

```

# Conclusion

