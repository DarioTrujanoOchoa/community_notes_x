---
title: "My UCSB Machine Learning Project on Community Notes"
author: "Dario Trujano-Ochoa"
date: "Fall 2023"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Embarking on my Machine Learning (ML) project I am delving into the world of Community Notes (CN) on X, formerly known as Twitter. 
I will use ML tools to find the best model to predict the number of ratings that each notes has received.

Community Notes, found on platforms like X (formerly Twitter), play a crucial role in combating misinformation and improving content moderation. They allow users to add context to posts, providing diverse viewpoints to counter potential biases. The algorithm guiding Community Notes emphasizes consensus, ensuring that it's not just about majority agreement. This collaborative approach empowers users to contribute to a more informed online space and brings transparency to the fact-checking process. The significance of Community Notes lies in their ability to debunk misinformation, lessen the impact of misleading content, and encourage a collective effort toward promoting accuracy in digital conversations.

![Fig 1. Community Notes Logo^[By Community Notes - https://twitter.com/CommunityNotes/photo, Public Domain, https://commons.wikimedia.org/w/index.php?curid=141534850]](images/Community_Notes_logo.png){ width=30%, style="display: block; margin: 0 auto;" }

## What are Community Notes?

Community Notes stands out as an innovative platform where contributors collaboratively add context to potentially misleading posts, challenging conventional content moderation methods. The publication of a CN is driven not by a majority rule but by the agreement of contributors who have previously disagreed, creating a transparent, community-driven approach to combat misinformation.

This sounds like a great idea, but it has been proven sometimes good but insufficient^[https://www.lemonde.fr/en/pixels/article/2023/07/03/i-spent-one-week-as-an-arbiter-of-truth-on-twitter-s-community-notes-service_6042188_13.html] or irrelevant^[https://mashable.com/article/twitter-x-community-notes-misinformation-views-investigation], and even susceptible to disinformation^[https://www.wired.com/story/x-community-notes-failures/], as you can see in more detail in the Wikipedia page dedicated to [Community Notes](https://en.wikipedia.org/wiki/Community_Notes#Criticisms_and_analysis) (CN).

![Fig 2. Community Notes Logo^[By Community Notes - https://github.com/twitter/communitynotes]](images/help-rate-this-note-expanded.png){ width=30%, style="display: block; margin: 0 auto;" }

## How are Notes posted?

At the core of this exploration is the open-source algorithm powering CN, described as ["insanely complicated."](https://uk.finance.yahoo.com/news/bird-watching-going-x-twitter-111442959.html?guccounter=1&guce_referrer=aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnLw&guce_referrer_sig=AQAAAJdoz-CTbAiZoR9yDHum0zUXTqVwhTXSB93ig32XLjzMO5rwCrk9QPJbcaqLCfwL2PcxqZgwd9zWLatoZAJkejGVwDHysvbYkRqcylDIefkFpoRgHDr1O4pvWFXlDV1Dfox1QJGZbGqJsszkp4VCvs_OOEGJVG6QqtL1QyDNA2tZ) This algorithm ensures that notes are rated by a diverse range of perspectives, incorporating an opinion classification based on contributors' alignment with the left and right-wing political spectrum. 
It is only after people that previously disagree, agree on the helpfulness of a note that the note is posted.
Therefore, the number of ratings that a note receives is very important to determine if the note is ever published, and how fast.

![Fig 3. Community Notes Rating)^[By Twitter - Original publication: Screenshot from CommunityNotesContributorImmediate source: https://twitter.com/i/communitynotes, Fair use, https://en.wikipedia.org/w/index.php?curid=75348629]](images/CommunityNotesRating.png){width=50%, style="display: block; margin: 0 auto;"}

## What this project explore?

The project is centered around a vast data set comprising around 380,000 notes, each representing a collaborative effort to combat misinformation. Of particular significance is the attempt to predict the number of ratings received by each note, as this is a crucial determinant in deciding whether a note is published. This predictive aspect adds a layer of complexity to our analysis, aiming to uncover insights into the collaborative evaluation system and its impact on the publication of notes.

This notes can be related to any topic and even advertising. 
It is worth noting that the most rated notes was about a [game](https://twitter.com/Evony_TKR/status/1672908357081124864).

The openness of the data invite scrutiny and analysis, fostering an environment where skepticism can be transformed into informed inquiry. 
Join me on this journey as we explore the intricacies of Community Notes.

# Data Set

The data from the notes and the ratings are open to anyone with an account on X. On the github page of CN you can also find the code and algorithm. 
Here are the sources:

- Data from the project from X, Community Notes, can be found [here](https://twitter.com/i/communitynotes/download-data).

  - The explanation of the data can be found [here](https://communitynotes.twitter.com/guide/en/under-the-hood/download-data).

- And the code from Community Notes is in [Github](https://github.com/twitter/communitynotes).

Since the data sets are very large, I save the final data set with the information I needed from each one. 
In this section I explain the original data sets and the creation of the final merged data used for the present project. 

## Original Open Source Data and Preparation for Analysis

In this section, I present the code I used to create and mere the raw data to create the final dataset.
If you want to replicate the code, just crate a folder named `data` on your working directory and download all the data directly from X. 
This section could be skipped and you can continue to the EDA where I work with the final merged data set. 

The raw data can be [downloaded](https://twitter.com/i/communitynotes/download-data) directly from X, and they provide a good [description](https://communitynotes.twitter.com/guide/en/under-the-hood/download-data) of all the variables in each data set. 
They update the data continuously. 
For this project all the data was downloaded December 3rd.
I will provide a codebook of the final data set I created from the raw data as documentation.

### Notes

You have to download the file: *notes-00000.tsv* 

```{r, eval=FALSE}
# packages
library(pacman)
p_load(tidyverse, 
       lubridate,
       naniar,
       janitor,
       forcats)

rm(list = ls())

# load data ----
# all the data was downloaded on December 3rd 2023
# notes
notes <- read_tsv("data/notes-00000.tsv") %>% clean_names()

## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
  notes %>% select(
    note_id,
    tweet_id,
    classification,
     trustworthy_sources,
     summary, 
    is_media_note,
    created_at_millis
     ) %>% 
  mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>% 
  mutate(w_day = wday(created_at, label = T),
         hour = as_factor(hour(created_at)),
         note_length = nchar(summary)) %>% 
  select(-c(created_at_millis,
            summary))


```

### Status

You have to download the file: *noteStatusHistory-00000.tsv* 

```{r, eval=FALSE}
## status
status <- read_tsv("data/noteStatusHistory-00000.tsv") %>% clean_names()

# the observations in the dataset are almost unique
length(unique(status$note_id))
# however they are all rated as "NEED MORE RATINGS"
duplicated_notes_status <-
status %>% group_by(note_id) %>% 
  summarise(n_notes = n()) %>% 
  filter(n_notes>1) %>% 
  pull(note_id) %>% 
  format(scientific = F)
status %>% filter(note_id %in% duplicated_notes_status) %>% View()

## select variables from ratings at the notes level ----
rates_summarise <-
bind_rows(r0, 
          r1,
          r2, 
          r3) %>% 
  group_by(note_id) %>% 
  summarise(
            ratings = n(),
            agreement_rate = sum(agree)/n(),
            helpful_rate = sum(helpfulness_level =="HELPFUL",na.rm = T)/n(),
            not_helpful_rate = sum(helpfulness_level =="NOT_HELPFUL",na.rm = T)/n(),
            somewhat_helpful_rate = sum(helpfulness_level =="SOMEWHAT_HELPFUL",na.rm = T)/n()
            )
```

### Ratings

You have to download the files: *ratings-00000.tsv*, *ratings-00001.tsv*, *ratings-00002.tsv*, *ratings-00003.tsv*.

```{r, eval=FALSE}
## ratings
r0 <- read_tsv("data/ratings-00000.tsv") %>% clean_names()
r1 <- read_tsv("data/ratings-00001.tsv") %>% clean_names()
r2 <- read_tsv("data/ratings-00002.tsv") %>% clean_names()
r3 <- read_tsv("data/ratings-00003.tsv") %>% clean_names()

## select variables from ratings at the notes level ----
rates_summarise <-
bind_rows(r0, 
          r1,
          r2, 
          r3) %>% 
  group_by(note_id) %>% 
  summarise(
            ratings = n(),
            agreement_rate = sum(agree)/n(),
            helpful_rate = sum(helpfulness_level =="HELPFUL",na.rm = T)/n(),
            not_helpful_rate = sum(helpfulness_level =="NOT_HELPFUL",na.rm = T)/n(),
            somewhat_helpful_rate = sum(helpfulness_level =="SOMEWHAT_HELPFUL",na.rm = T)/n()
            )

```

### Data Preparation: Merging the Data

The previous data sets will be merged in a file named `notes_merged.RData`. 
This data set contains all the variables used in the analysis, and their description can be found in the codebook. 

```{r, eval=FALSE}
# merge data ----
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(note_id)) %>% 
  # some notes never received ratings, let's replace them with 0
  replace_na(list(ratings = 0,
                  agreement_rate = 0,
                  helpful_rate = 0,
                  not_helpful_rate = 0,
                  somewhat_helpful_rate = 0)) 
notes_merged <- 
  left_join(x = notes_merged, 
            y = status %>% 
              # select onlye the non duplicated rows
              filter(!(note_id %in% duplicated_notes_status)) %>% 
              # I only analyze the current status
              select(note_id,current_status), 
            by = join_by(note_id))

save(notes_merged,file = "data/notes_merged.RData")
```

# EDA

```{r Load Packages}
# Load Packages
library(pacman)
p_load(tidyverse, 
       tidymodels, 
       recipes,
       kknn,
       yardstick,
       tune,
       ggplot2,
       ggthemes,
       rsample,
       parsnip,
       workflows,
       corrplot
       )

load("../data/notes_merged.RData")
```

## Missing data

In the data set there is only 3 values with missing data. 
When I looked for these specific tweets there were nothing posted. I assume this were mistakes.
Given the magnitude of the data set it won't affect the analysis if we just remove them.
The ID of the notes with missing data are:

```{r}
# missing data ----
# there are 3 rows with missing values
missing_cell <- which(is.na(notes_merged), arr.ind = TRUE)
# These are the tweet ids
notes_merged[missing_cell[,1],] %>% select(tweet_id) %>% pull() %>% format(scientific = F) %>% unique()
# There is no summary in this notes, probably this was a mistake
# There is nothing in the note 1370110240532930560 that had 8 ratings. The other two notes were never rated.

# I remove the missing values, given the content and the number of missing values this shouldn't be an issue
notes_merged <- notes_merged %>% drop_na()
```

## Outcome variable: Number of Ratings

Let's first check the summary statistics of the number of ratings in each note:

```{r}
# Analyzing the outcome variable ----
summary(notes_merged$ratings)
```

There is a lot of variability, and it is clear that many notes receive a lot of attention.
The median is far less than the mean. 

Lets check this looking at the histograms. 
I separate the notes by the lowest 99% and the highest 1% by number of ratings.
We can see that many notes are never rated, but many notes receive some level of attention. 
For the most popular ones, it is clear that one note creates a lot of distortion.

```{r, message=FALSE}
# 99% of the notes have less than 453 ratings
q_99 <- quantile(notes_merged$ratings,probs = 0.99)

notes_merged %>% 
  filter(ratings < q_99) %>% 
  ggplot() +
  geom_histogram(aes(x= ratings)) + 
  labs(
    title = "Histogram of the number of Ratings on each Note",
    subtitle = "Percentile 99 of the Ratings",
    x="Number of Ratings"
  ) +
  theme_bw()

notes_merged %>% 
  filter(ratings >= q_99) %>% 
  ggplot() +
  geom_histogram(aes(x= ratings)) + 
  labs(
    title = "Histogram of the number of Ratings on each Note",
    subtitle = "1% of Notes with more Ratings",
    x="Number of Ratings"
  ) +
  theme_bw()
```


The note with the most ratings is about [advertising](https://twitter.com/anyuser/status/1672908357081124864):

```{r}
# The note with most ratings
notes_merged %>% filter(ratings>6000) %>% arrange(ratings) %>% 
  select(tweet_id) %>% 
  pull() %>% 
  format(scientific = F)
```

## Correlations

```{r}
# Correlations ----
# Correlation matrix
notes_cor <- cor(notes_merged %>% 
                   select(-ends_with("id")) %>% 
                   select_if(is.numeric))

# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor, 
                                 lower = 'shade', upper = 'pie', order = 'hclust', 
                                 addCoef.col = 1, number.cex = 0.7,
                                 tl.pos = "lt"
                                 )
```

The strongest correlations are between the length of the note and if the note has a trustworthy source (`r cor(notes_merged$note_length, notes_merged$trustworthy_sources)`), if the note has rated as helpful or not helpful (`r cor(notes_merged$helpful_rate, notes_merged$not_helpful_rate)`), the agreement rate with the time (in milliseconds) the note was created (`r cor(notes_merged$agreement_rate, notes_merged$created_at_millis)`), and the helpful level with the time the note was created (`r cor(notes_merged$helpful_rate, notes_merged$created_at_millis)`).
The correlations are in general very low between the numeric predictors.

The correlation of helpfulness with non helpfulness is expected since when user rate the only other option is "somewhat helpful". This is the reason the correlation is not perfectly negative. 
It is interesting to notice that the agreement decreased with the time the note was created, meaning that with more time of this policy, people started to agreed less, which is expected if more people if more people is participating, however the correlation between the time of creation and number of ratings is low. 
It is interesting that the time of creation is at the same time increasing the both the helpful and non helpful rates. 
This might be related to the fact that people have become more certain in their ratings since less people is selecting the somewhat helpful option. 

## Relationship between Ratings and Classification 

It seems that more ratings are associated with the note rated as helpful, which is expected from how the algorithm is described.
Also, it is notes classified as not "helpful" receive more ratings.
This is true for looking at the mean, meadian and 10th and 90th quantile.

```{r}
rmarkdown::paged_table(
  notes_merged %>% group_by(current_status) %>% 
  summarise(mean(ratings), median(ratings), 
            quantile(ratings,probs = 0.1), 
            quantile(ratings,probs = 0.9)
            )
)
```

In terms of what the note says about the tweet, more notes say that the tweet is "MISINFORMED_OR_POTENTIALLY_MISLEADING".
This is specially marked for the notes rated as "HELPFUL" were virtually all the notes say the tweet was misinformed or potentially misleading.

```{r}
rmarkdown::paged_table(
  as.data.frame(
    table(notes_merged$classification, notes_merged$current_status)) %>%
    pivot_wider(names_from = Var1,values_from = Freq) %>% 
    rename("Current State" = Var2)
)
```

Finally, the number of ratings is very similar between the classification of notes. 

```{r}
rmarkdown::paged_table(
  notes_merged %>% group_by(classification) %>% 
  summarise(mean(ratings), median(ratings), 
            quantile(ratings,probs = 0.1), 
            quantile(ratings,probs = 0.9)
            )
)
```
# Final Data and Models

```{r}
rmarkdown::paged_table(
  notes_merged
)
```


Now that I have the final data set `notes_merged`, I can start the analysis.
We have to import some packages including `tidymodels`, `yardstick` and `tune`.

```{r}
# Load Packages
library(pacman)
p_load(tidyverse, 
       tidymodels, 
       recipes,
       kknn,
       yardstick,
       tune,
       ggplot2,
       ggthemes,
       rsample,
       parsnip,
       workflows
       )
```

## Setting up the Data

### Data Splitting and Stratification

Now, let's split the original data set to make the analysis.
I decided to use $75\%$ of the data for training, and the sampling is stratified at the outcome variable `ratings`.  

```{r}
# To reproduce the results
set.seed(1984)

# Percentage used for the training set
training_percentage <- 0.75

# Splitting the data
split_notes <- initial_split(notes_merged,
                             prop = training_percentage,
                             strata = ratings)
train_notes <- training(split_notes)
test_notes <- testing(split_notes)
```

The proportion of observations in the training set was `r nrow(train_notes)/nrow(notes_merged)`, and `r nrow(test_notes)/nrow(notes_merged)` for the test set.
These numbers are closed to the proportion stated.

In the context of Community Notes within machine learning, envisioning the data set as a collection of notes, the process of dividing this data into training, testing, and validation sets becomes analogous to strategizing how to understand and predict the behavior of future notes. The existing notes serve as a sample, providing insights into how contributors have added context to posts in the past. However, it's imperative not to assume that the future usage of notes will precisely mirror historical patterns.

Much like a training set, a substantial portion of existing notes would be allocated to allow the model to learn patterns, relationships, and features inherent in the data. This phase involves understanding how contributors have historically interacted with posts, detecting common themes, and learning the dynamics of note creation. The testing set, representative of notes yet unseen by the model, acts as a simulated evaluation of the model's ability to generalize its learning to new instances of notes. This evaluation is critical in anticipating how well the model would adapt to future notes scenarios.

To account for the unpredictability and potential evolution in how contributors may use CN in the future, a validation set becomes paramount. This set serves as a means of fine-tuning the model, preventing it from over fitting the historical data and ensuring that it doesn't make assumptions based solely on past patterns. The aim is to create a model that is not only proficient in understanding the existing CN but is also equipped to adapt to unforeseen events and new patterns that may emerge in future note creation.

### Cross Validation

```{r}
notes_folds <- vfold_cv(train_notes, v = 10, strata = "ratings")
```


I also  divide the training data from Community Notes into 10 folds to perform a K-Fold Cross Validation.
This is an advanced technique employed to rigorously evaluate the performance of a machine learning model. The process commences by partitioning the dataset into ten subsets, ensuring an equitable distribution of the Community Notes data. Subsequently, the model undergoes ten iterations of training and testing, where each fold is sequentially designated as the testing set, and the remaining folds collectively form the training set.

During each iteration, the machine learning model is trained on the training set, enabling it to discern intricate patterns and nuances within the Community Notes data. The model's proficiency is then rigorously tested on the designated testing set, elucidating its capacity to generalize across diverse subsets of the dataset. Performance metrics, encompassing accuracy, precision, and recall, are meticulously recorded for each iteration, furnishing a granular assessment of the model's efficacy.

The ultimate evaluation derives from averaging the performance metrics across all ten iterations. This calculated average serves as a comprehensive measure of the model's generalization prowess, providing a nuanced understanding of its robustness in handling the intricacies and variations inherent in the diverse landscape of Community Notes. In essence, the adoption of K-Fold Cross Validation with 10 folds at a graduate level ensures a thorough and reliable evaluation of the machine learning model's competence in understanding and predicting patterns within the multifaceted Community Notes dataset.

## Model Building

### Recipe,  Models and Workflow

```{r, eval=FALSE}
# Recipe ----
rec_reg <- recipe(ratings ~  ., 
              data = train_notes) %>% 
  step_rm(note_id, tweet_id, 
          created_at, current_status) %>% 
  step_dummy(w_day, hour, classification) %>% 
  step_normalize(agreement_rate) 
```

```{r}
# Models ----
##linear model ----
linear_reg <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

# KNN model
knn_mod <- nearest_neighbor(neighbors = tune()) %>%
  set_mode("regression") %>%
  set_engine("kknn")

## Elastic net ----
# Tuning penalty and mixture
en_mod <- linear_reg(penalty = tune(), 
                           mixture = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")

## Random forest ----
# Tuning mtry (number of predictors), trees, and min_n (number of minimum values in each node)
rf_mod <- rand_forest(mtry = tune(), 
                       trees = tune(), 
                       min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

```


```{r, eval=FALSE}
# Workflow ----
## Linear ----
lm_wkflow <- workflow() %>% 
  # add model
  add_model(linear_reg) %>% 
  # add receipe
  add_recipe(rec_reg)

## KNN ----
knn_wkflow <- workflow() %>% 
  # add model
  add_model(knn_mod) %>% 
  # add receipe
  add_recipe(rec_reg)

## EN ----
en_wkflow <- workflow() %>% 
  # add model
  add_model(en_mod) %>% 
  # add receipe
  add_recipe(rec_reg)

## RF ----
rf_wkflow <- workflow() %>% 
  # add model
  add_model(rf_mod) %>% 
  # add receipe
  add_recipe(rec_reg)

```


## Tuning the Parameters

```{r, eval=FALSE}
# Grids for tunning parameters ----
# For the Linear model there is no parameter to tune, so I don't need a grid or tuning.
## KNN ----
knn_grid <- grid_regular(neighbors(range = c(1,15)), 
                         levels = 5)

## EN ----
en_grid <- grid_regular(penalty(range = c(-5, 5)), 
                             mixture(range = c(0,1)), 
                             levels = 10)

## RF ----
rf_grid <- grid_regular(mtry(range = c(1, 12)), 
                                  trees(range = c(200,1000)), 
                                  min_n(range = c(5,20)), 
                                  levels = 8)

# Tuning ----
## KKN ----
knn_tune <- tune_grid(
  knn_wkflow,
  resamples = notes_folds,
  grid = knn_grid
)

## EN ----
en_tune <- tune_grid(
  en_wkflow,
  resamples = notes_folds,
  grid = en_grid
)

## RF ----
rf_tune_res <- tune_grid(
  rf_wkflow,
  resamples = notes_folds,
  grid = rf_grid
)

# Save tuning results ----

## KNN ----
write_rds(knn_tune, file = "data/tuned_models/knn.rds")

## EN ----
write_rds(elastic_tune, file = "data/tuned_models/elastic.rds")

## RF ----
write_rds(rf_tune_res, file = "data/tuned_models/rf.rds")

```

Finally, we save all the results from the tuning process. 
For the present document, we used the data stored in this files instead of running all the analysis again when knitting the Rmd file.

```{r, eval=FALSE}
# Load tunning results ----
## KNN ----
read_rds(knn_tune, file = "data/tuned_models/knn.rds")

## EN ----
read_rds(elastic_tune, file = "data/tuned_models/elastic.rds")

## RF ----
read_rds(rf_tune_res, file = "data/tuned_models/rf.rds")
```


# Best Model

```{r}

```


# Conclusion

The variable `agreement_rate` had little variability. For the sample 

