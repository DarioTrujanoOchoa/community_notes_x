not_helpful = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)
)
<<<<<<< Updated upstream
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId))
notes_merged
save(notes_merged,file = "data/notes_merged.RData")
load("data/notes_merged.RData")
vis_miss(s_notes)
vis_miss(s_notes==0)
# The variables Believable, harmful, and validation difficulty were Deprecated as of 2022-10-27.
vis_miss(notes_merged)
# The variables Believable, harmful, and validation difficulty were Deprecated as of 2022-10-27.
vis_miss(slice_sample(notes_merged,prop = 0.1))
set.seed(1984)
training_percentage <- 0.75
split_notes <- initial_split(notes_merged,prop = training_percentage)
training_notes <- training(split_notes)
test_notes <- testing(split_notes)
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune)
p_load(ggplot2)
p_load(ggthemes)
split_notes <- initial_split(notes_merged,prop = training_percentage)
training_notes <- training(split_notes)
test_notes <- testing(split_notes)
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes)
split_notes <- initial_split(notes_merged,prop = training_percentage)
?initial_split
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes)
library(tidymodels)
pacman::p_load(ggplot2)
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes)
library(pacman)
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune)
p_load(ggplot2)
p_load(ggthemes)
set.seed(1984)
training_percentage <- 0.75
split_abalone <- initial_split(abalone,prop = training_percentage, strata = type)
training_abalone <- training(split_abalone)
test_abalone <- testing(split_abalone)
library(tidymodels)
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
# ggplot2,
ggthemes)
=======
rates_summarise
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId))
notes_merged
save(notes_merged,file = "data/notes_merged.RData")
lm(ratings ~
classification +
trustworthySources +
note_length,
data = notes_merged)
p_load(tidyverse,
lubridate,
naniar)
>>>>>>> Stashed changes
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes,
<<<<<<< Updated upstream
rsample)
split_notes <- initial_split(notes_merged,prop = training_percentage)
training_notes <- training(split_notes)
test_notes <- testing(split_notes)
=======
rsample,
parsnip,
workflows
)
load("data/notes_merged.RData")
>>>>>>> Stashed changes
vis_miss(slice_sample(notes_merged,prop = 0.1))
set.seed(1984)
training_percentage <- 0.75
split_notes <- initial_split(notes_merged,prop = training_percentage)
training_notes <- training(split_notes)
test_notes <- testing(split_notes)
<<<<<<< Updated upstream
# recipe
rec_reg <- recipe(ratings ~  .,
data = notes_merged %>% select(-c(noteId,summary)))
# linear model
linear_reg <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
?set_engine
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes,
rsample,
parsnip
)
# linear model
linear_reg <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
# Workflow ----
## empty workflow
lm_wkflow <- workflow() %>%
# add model
add_model(linear_reg) %>%
# add receipe
add_recipe(rec_reg)
?add_recipe
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes,
rsample,
parsnip,
workflows
)
# Workflow ----
## empty workflow
lm_wkflow <- workflow() %>%
# add model
add_model(linear_reg) %>%
# add receipe
add_recipe(rec_reg)
# fitting models
fit_lm <-
lm_wkflow %>%
fit(data = training_abalone)
# recipe ----
rec_reg <- recipe(ratings ~  .,
data = training_notes %>% select(-c(noteId,summary)))
# linear model ----
linear_reg <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
# Workflow ----
## empty workflow
lm_wkflow <- workflow() %>%
# add model
add_model(linear_reg) %>%
# add receipe
add_recipe(rec_reg)
# fitting models
fit_lm <-
lm_wkflow %>%
fit(data = training_notes)
fit_lm
# metrics
notes_metrics <- metric_set(rmse, rsq, mae)
# linear model
notes_lm_aug <- augment(fit_lm,test_abalone)
# linear model
notes_lm_aug <- augment(fit_lm, test_notes)
notes_metrics(notes_lm_aug, truth = age,
estimate = .pred)
# linear model
notes_lm_aug <- augment(fit_lm, test_notes)
notes_metrics(notes_lm_aug, truth = ratings,
estimate = .pred)
fit_lm
=======
>>>>>>> Stashed changes
# recipe ----
rec_reg <- recipe(ratings ~  .,
data = training_notes %>% select(-c(noteId,summary))) %>%
step_normalize(agreement_rate)
<<<<<<< Updated upstream
# linear model ----
linear_reg <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
# Workflow ----
## empty workflow
lm_wkflow <- workflow() %>%
# add model
add_model(linear_reg) %>%
# add receipe
add_recipe(rec_reg)
# fitting models
fit_lm <-
lm_wkflow %>%
fit(data = training_notes)
fit_lm
# metrics
notes_metrics <- metric_set(rmse, rsq, mae)
# linear model
notes_lm_aug <- augment(fit_lm, test_notes)
notes_metrics(notes_lm_aug, truth = ratings,
estimate = .pred)
?metric_set
notes_merged %>% ggplot() %>%
notes_merged %>% ggplot() +
geom_point(aes(x=agreement_rate,y=ratings))
=======
# models ----
##linear model ----
linear_reg <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
# KNN model
k = 7
>>>>>>> Stashed changes
knn_mod <- nearest_neighbor(neighbors = k) %>%
set_mode("regression") %>%
set_engine("kknn")
# Workflow ----
## linear ----
lm_wkflow <- workflow() %>%
# add model
add_model(linear_reg) %>%
# add receipe
add_recipe(rec_reg)
## KNNn model ----
knn_wkflow <- workflow() %>%
# add model
add_model(knn_mod) %>%
# add receipe
add_recipe(rec_reg)
<<<<<<< Updated upstream
## KNN ----
fit_knn <-
knn_wkflow %>%
fit(data = training_abalone)
## KNN ----
fit_knn <-
knn_wkflow %>%
fit(data = training_notes)
=======
# fitting models ----
## linear ----
fit_lm <-
lm_wkflow %>%
fit(data = training_notes)
fit_lm
>>>>>>> Stashed changes
## KNN ----
fit_knn <-
knn_wkflow %>%
fit(data = training_notes)
<<<<<<< Updated upstream
# KNN model
k = 7
knn_mod <- nearest_neighbor(neighbors = k) %>%
set_mode("regression") %>%
set_engine("kknn")
fit_knn
## KNN ----
fit_knn <-
knn_wkflow %>%
fit(data = training_notes)
=======
fit_knn
# metrics
notes_metrics <- metric_set(rmse, rsq, mae)
# linear model
notes_lm_aug <- augment(fit_lm, test_notes)
notes_metrics(notes_lm_aug, truth = ratings,
estimate = .pred)
notes_merged %>% ggplot() +
geom_point(aes(x=agreement_rate,y=ratings))
# linear model
notes_knn_aug <- augment(fit_knn, test_notes)
notes_metrics(notes_knn_aug, truth = ratings,
estimate = .pred)
fit_knn
# linear model
notes_lm_aug <- augment(fit_lm, test_notes)
fit_knn
# knn model
notes_knn_aug <- augment(fit_knn, test_notes)
notes_knn_aug
notes_metrics(notes_lm_aug, truth = ratings,
estimate = .pred)
>>>>>>> Stashed changes
library(pacman)
p_load(tidyverse,
lubridate,
naniar)
# load data ----
# notes
notes <- read_tsv("data/notes-00000.tsv")
spec(notes)
## status
status <- read_tsv("data/noteStatusHistory-00000.tsv")
spec(status)
## ratings
r0 <- read_tsv("data/ratings-00000.tsv")
r1 <- read_tsv("data/ratings-00001.tsv")
r2 <- read_tsv("data/ratings-00002.tsv")
r3 <- read_tsv("data/ratings-00003.tsv")
spec(r0)
# Almost all the notes in notes are in status
# only 20 notes are not there
sum(notes$noteId %in% status$noteId)
s_notes <- slice_sample(notes,n = 10000)
s_status <- slice_sample(status,n = 10000)
# The variables Believable, harmful, and validation difficulty were Deprecated as of 2022-10-27.
vis_miss(s_notes)
vis_miss(s_status)
# notes ----
## Authors ----
# the observations in the dataset are unique for each note
length(unique(notes$noteId))
# the Some notes are made by more than one participant
length(unique(s_notes$noteAuthorParticipantId))
# most users publish a note only once (60% in sample)
table(table(s_notes$noteAuthorParticipantId))
barplot(table(table(s_notes$noteAuthorParticipantId)),
xlab = "Number of Notes",
ylab = "Number of Authors",
main = "Distribution of Notes Published by Author")
# ID of prolific authors
head(sort(table(s_notes$noteAuthorParticipantId),decreasing = T),n = 25 )
## Tweets ----
# some tweets have more than one note
# most of the tweets have a single note
length(unique(notes$tweetId))
barplot(table(table(s_notes$tweetId)),
xlab = "Number of Notes",
ylab = "Number of Tweets",
main = "Distribution of Notes Published by Tweet")
# all the variables starting with misleading are answers to the reason something was misleading
# all variables starting with notMisleadnig are answers to the reason something was not misleading
# therefore cannot be used to explain why a note is misleading or not
as_tibble(s_notes) %>%
filter(classification == "MISINFORMED_OR_POTENTIALLY_MISLEADING") %>%
select_if(is.numeric) %>%
map_dbl(sum) %>% as.data.frame()
table(s_notes$classification,s_notes$notMisleadingOther)
table(s_notes$classification,s_notes$isMediaNote)
table(s_notes$classification,s_notes$trustworthySources)
# select the variables that will be used in the model from the notes dataset
notes_final <-
notes %>% select(
noteId,
classification,
trustworthySources,
summary,
isMediaNote
) %>%
mutate(note_length = nchar(summary))
# status ----
# the observations in the dataset are unique
length(unique(status$noteId))
# Contains -1 if the note never left “Needs More Ratings” status.
# most of the notes needed more ratings
sum(status$timestampMillisOfFirstNonNMRStatus==-1)
barplot(table(status$currentStatus))
table(status$currentStatus, status$lockedStatus)
# ratings ----
# there are 109,142 raters. Small for the number of notes.
# they cover
length(unique(r0$raterParticipantId))
# only 13 people in r3 were in r1
# 42 people in r1 were in r3
# There is no more repetitions
sum(r1$raterParticipantId %in%
r3$raterParticipantId)
barplot(table(table(r0$raterParticipantId)),
xlab = "Number of Ratings",
ylab = "Number of Raters",
main = "Distribution of Ratings Published by Rater")
## ratings in notes ----
# most of the notes are in the ratings dataset
sum(r0$noteId %in% notes$noteId)
sum(r1$noteId %in% notes$noteId)
sum(r2$noteId %in% notes$noteId)
sum(r3$noteId %in% notes$noteId)
mean(notes$noteId %in% r0$noteId)
mean(notes$noteId %in% r1$noteId)
mean(notes$noteId %in% r2$noteId)
mean(notes$noteId %in% r3$noteId)
mean(r3$noteId %in% r1$noteId)
# Most notes have few ratings
barplot(table(table(r0$noteId)),
xlab = "Number of Ratings",
ylab = "Number of Notes",
main = "Distribution of Ratings Published by Note")
table(r0$helpfulnessLevel)
rates_summarise <-
bind_rows(r0,
r1,
r2,
r3) %>%
group_by(noteId) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful = sum(helpfulnessLevel =="HELPFUL",na.rm = T),
not_helpful = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)
)
rates_summarise
# merge data ----
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId))
notes_merged
save(notes_merged,file = "data/notes_merged.RData")
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes,
rsample,
parsnip,
workflows
)
load("data/notes_merged.RData")
vis_miss(slice_sample(notes_merged,prop = 0.1))
set.seed(1984)
training_percentage <- 0.75
split_notes <- initial_split(notes_merged,prop = training_percentage)
training_notes <- training(split_notes)
test_notes <- testing(split_notes)
# recipe ----
rec_reg <- recipe(ratings ~  .,
data = training_notes %>% select(-c(noteId,summary))) %>%
step_normalize(agreement_rate)
# models ----
##linear model ----
linear_reg <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
# KNN model
k = 7
knn_mod <- nearest_neighbor(neighbors = k) %>%
set_mode("regression") %>%
set_engine("kknn")
# Workflow ----
## linear ----
lm_wkflow <- workflow() %>%
# add model
add_model(linear_reg) %>%
# add receipe
add_recipe(rec_reg)
## KNNn model ----
knn_wkflow <- workflow() %>%
# add model
add_model(knn_mod) %>%
# add receipe
add_recipe(rec_reg)
# fitting models ----
## linear ----
fit_lm <-
lm_wkflow %>%
fit(data = training_notes)
fit_lm
## KNN ----
fit_knn <-
knn_wkflow %>%
fit(data = training_notes)
fit_knn
# metrics
notes_metrics <- metric_set(rmse, rsq, mae)
# linear model
notes_lm_aug <- augment(fit_lm, test_notes)
notes_metrics(notes_lm_aug, truth = ratings,
estimate = .pred)
notes_merged %>% ggplot() +
geom_point(aes(x=agreement_rate,y=ratings))
# knn model
notes_knn_aug <- augment(fit_knn, test_notes)
notes_metrics(notes_knn_aug, truth = ratings,
estimate = .pred)
