<<<<<<< Updated upstream
=======
sum(r1$noteId %in% notes$noteId)
sum(r2$noteId %in% notes$noteId)
sum(r3$noteId %in% notes$noteId)
mean(notes$noteId %in% r0$noteId)
mean(notes$noteId %in% r1$noteId)
mean(notes$noteId %in% r2$noteId)
mean(notes$noteId %in% r3$noteId)
>>>>>>> Stashed changes
mean(r3$noteId %in% r1$noteId)
# Most notes have few ratings
barplot(table(table(r0$noteId)),
xlab = "Number of Ratings",
ylab = "Number of Notes",
main = "Distribution of Ratings Published by Note")
table(r0$helpfulnessLevel)
rates_summarise <-
bind_rows(r0,
r1,
r2,
r3) %>%
group_by(noteId) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful = sum(helpfulnessLevel =="HELPFUL",na.rm = T),
not_helpful = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)
)
rates_summarise
# merge data ----
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId))
notes_merged
save(notes_merged,file = "data/notes_merged.RData")
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes,
rsample,
parsnip,
workflows
)
load("data/notes_merged.RData")
vis_miss(slice_sample(notes_merged,prop = 0.1))
set.seed(1984)
training_percentage <- 0.75
split_notes <- initial_split(notes_merged,prop = training_percentage)
training_notes <- training(split_notes)
test_notes <- testing(split_notes)
# recipe ----
rec_reg <- recipe(ratings ~  .,
data = training_notes %>% select(-c(noteId,summary))) %>%
step_normalize(agreement_rate)
# models ----
##linear model ----
linear_reg <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
# KNN model
k = 7
knn_mod <- nearest_neighbor(neighbors = k) %>%
set_mode("regression") %>%
set_engine("kknn")
# Workflow ----
## linear ----
lm_wkflow <- workflow() %>%
# add model
add_model(linear_reg) %>%
# add receipe
add_recipe(rec_reg)
## KNNn model ----
knn_wkflow <- workflow() %>%
# add model
add_model(knn_mod) %>%
# add receipe
add_recipe(rec_reg)
# fitting models ----
## linear ----
fit_lm <-
lm_wkflow %>%
fit(data = training_notes)
fit_lm
## KNN ----
fit_knn <-
knn_wkflow %>%
fit(data = training_notes)
fit_knn
# metrics
notes_metrics <- metric_set(rmse, rsq, mae)
# linear model
notes_lm_aug <- augment(fit_lm, test_notes)
notes_metrics(notes_lm_aug, truth = ratings,
estimate = .pred)
notes_merged %>% ggplot() +
geom_point(aes(x=agreement_rate,y=ratings))
# knn model
notes_knn_aug <- augment(fit_knn, test_notes)
notes_metrics(notes_knn_aug, truth = ratings,
estimate = .pred)
<<<<<<< Updated upstream
View(notes_merged)
load("data/notes_merged.RData")
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary))
# packages
library(pacman)
p_load(tidyverse,
lubridate,
naniar,
janitor,
forcats)
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary))
# load data ----
# all the data was downloaded on December 3rd 2023
# notes
notes <- read_tsv("data/notes-00000.tsv") %>% clean_names()
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary))
# merge data ----
# Notes in both data sets are not exactly the same
# 9.3% of the notes never received ratings
1 - mean(notes_final$note_id %in% rates_summarise$note_id)
notes_final$note_id
# merge data ----
# Notes in both data sets are not exactly the same
# 9.3% of the notes never received ratings
1 - mean(notes_final$note_id %in% rates_summarise$note_id)
r1 <- read_tsv("data/ratings-00001.tsv") %>% clean_names()
names(notes_merged)
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary)) %>%
select(-created_at_millis)
# There is no missing values
vis_miss(slice_sample(notes_final,prop = 0.1))
## status
status <- read_tsv("data/noteStatusHistory-00000.tsv") %>% clean_names()
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary)) %>%
select(-created_at_millis)
## select variables from ratings at the notes level ----
rates_summarise <-
bind_rows(r0,
r1,
r2,
r3) %>%
group_by(note_id) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulness_level =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulness_level =="NOT_HELPFUL",na.rm = T)/n(),
somewhat_helpful_rate = sum(helpfulness_level =="SOMEWHAT_HELPFUL",na.rm = T)/n()
)
# merge data ----
# Notes in both data sets are not exactly the same
# 9.3% of the notes never received ratings
1 - mean(notes_final$note_id %in% rates_summarise$note_id)
rates_summarise
## select variables from ratings at the notes level ----
rates_summarise <-
bind_rows(r0,
r1,
r2,
r3) %>%
group_by(note_id) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulness_level =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulness_level =="NOT_HELPFUL",na.rm = T)/n(),
somewhat_helpful_rate = sum(helpfulness_level =="SOMEWHAT_HELPFUL",na.rm = T)/n()
)
## select variables from ratings at the notes level ----
rates_summarise <-
bind_rows(r0,
r1,
#r2,
r3) %>%
group_by(note_id) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulness_level =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulness_level =="NOT_HELPFUL",na.rm = T)/n(),
somewhat_helpful_rate = sum(helpfulness_level =="SOMEWHAT_HELPFUL",na.rm = T)/n()
)
# merge data ----
# Notes in both data sets are not exactly the same
# 9.3% of the notes never received ratings
1 - mean(notes_final$note_id %in% rates_summarise$note_id)
## select variables from ratings at the notes level ----
rates_summarise <-
bind_rows(r0,
#r1,
r2,
r3) %>%
group_by(note_id) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulness_level =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulness_level =="NOT_HELPFUL",na.rm = T)/n(),
somewhat_helpful_rate = sum(helpfulness_level =="SOMEWHAT_HELPFUL",na.rm = T)/n()
)
# merge data ----
# Notes in both data sets are not exactly the same
# 9.3% of the notes never received ratings
1 - mean(notes_final$note_id %in% rates_summarise$note_id)
r1 <- read_tsv("data/ratings-00001.tsv") %>% clean_names()
r2 <- read_tsv("data/ratings-00002.tsv") %>% clean_names()
r3 <- read_tsv("data/ratings-00003.tsv") %>% clean_names()
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary)) %>%
select(-c(created_at_millis,
summary))
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary)) %>%
select(-c(created_at_millis,
summary))
names(notes_final)
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary)) %>%
select(-c(created_at_millis,
summary))
## select variables from ratings at the notes level ----
rates_summarise <-
bind_rows(r0,
r1,
r2,
r3) %>%
group_by(note_id) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulness_level =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulness_level =="NOT_HELPFUL",na.rm = T)/n(),
somewhat_helpful_rate = sum(helpfulness_level =="SOMEWHAT_HELPFUL",na.rm = T)/n()
)
## select variables from ratings at the notes level ----
rates_summarise <-
bind_rows(r0,
#r1,
r2,
r3) %>%
group_by(note_id) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulness_level =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulness_level =="NOT_HELPFUL",na.rm = T)/n(),
somewhat_helpful_rate = sum(helpfulness_level =="SOMEWHAT_HELPFUL",na.rm = T)/n()
)
# merge data ----
# Notes in both data sets are not exactly the same
# 9.3% of the notes never received ratings
1 - mean(notes_final$note_id %in% rates_summarise$note_id)
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(note_id)) %>%
# some notes never received ratings, let's replace them with 0
=======
names(notes_merged)
rates_summarise <-
bind_rows(r0,
r1,
r2,
r3) %>%
group_by(noteId) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulnessLevel =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)/n()
)
library(pacman)
p_load(tidyverse,
lubridate,
naniar)
# there are 109,142 raters. Small for the number of notes.
# they cover
length(unique(r0$raterParticipantId))
# only 13 people in r3 were in r1
# 42 people in r1 were in r3
# There is no more repetitions
sum(r1$raterParticipantId %in%
r3$raterParticipantId)
barplot(table(table(r0$raterParticipantId)),
xlab = "Number of Ratings",
ylab = "Number of Raters",
main = "Distribution of Ratings Published by Rater")
## ratings in notes ----
# most of the notes are in the ratings dataset
sum(r0$noteId %in% notes$noteId)
sum(r1$noteId %in% notes$noteId)
sum(r2$noteId %in% notes$noteId)
sum(r3$noteId %in% notes$noteId)
mean(notes$noteId %in% r0$noteId)
mean(notes$noteId %in% r1$noteId)
mean(notes$noteId %in% r2$noteId)
mean(notes$noteId %in% r3$noteId)
mean(r3$noteId %in% r1$noteId)
# Most notes have few ratings
barplot(table(table(r0$noteId)),
xlab = "Number of Ratings",
ylab = "Number of Notes",
main = "Distribution of Ratings Published by Note")
table(r0$helpfulnessLevel)
rates_summarise <-
bind_rows(r0,
r1,
r2,
r3) %>%
group_by(noteId) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulnessLevel =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)/n()
)
rates_summarise <-
bind_rows(r0,
r1,
r2,
r3) %>%
group_by(noteId) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulnessLevel =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)/n(),
somewhat_helpful_rate = sum(helpfulnessLevel =="SOMEWHAT_HELPFUL",na.rm = T)/n()
)
rates_summarise
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId))
notes_merged
save(notes_merged,file = "data/notes_merged.RData")
p_load(tidyverse,
ggplo2)
p_load(tidyverse,
lubridate,
naniar,
janitor)
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId)) %>%
clean_names()
notes_merged
save(notes_merged,file = "data/notes_merged.RData")
vis_miss(slice_sample(notes_merged,prop = 0.1))
vis_miss(slice_sample(rates_summarise,prop = 0.1))
# missing values
vis_miss(slice_sample(notes_final,prop = 0.1))
# missing data
vis_miss(slice_sample(notes_merged,prop = 0.1))
# some notes never received ratings
(notes_final %in% rates_summarise)
# some notes never received ratings
mean(notes_final %in% rates_summarise)
# some notes never received ratings
mean(notes_final$noteId %in% rates_summarise$noteId)
# 90.65% of the notes never received ratings
1 - mean(notes_final$noteId %in% rates_summarise$noteId)
# merge data ----
# Notes in both data sets are not exactly the same
# 9.3% of the notes never received ratings
1 - mean(notes_final$noteId %in% rates_summarise$noteId)
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId)) %>%
clean_names() %>% # let's clean the data names
# some notes never received ratings, lte's replace them with 0
replace_na(list(ratings = 0,
agreement_rate = 0,
helpful_rate = 0,
not_helpful_rate = 0,
somewhat_helpful_rate = 0))
notes_merged
save(notes_merged,file = "data/notes_merged.RData")
# missing data
vis_miss(slice_sample(notes_merged,prop = 0.1))
notes_merged %>%
ggplot() +
geom_histogram(aes(x= ratings))
summary(notes_merged$ratings)
mean(notes_merged$ratings>100)
mean(notes_merged$ratings>1000)
mean(notes_merged$ratings>500)
mean(notes_merged$ratings>200)
mean(notes_merged$ratings>400)
mean(notes_merged$ratings>500)
notes_merged %>%
filter(ratings < 500)
notes_merged %>%
filter(ratings < 500) %>%
ggplot() +
geom_histogram(aes(x= ratings))
# Analyzing the outcome variable
summary(notes_merged$ratings)
notes_merged %>%
filter(ratings > 500) %>%
ggplot() +
geom_histogram(aes(x= ratings))
# select the variables that will be used in the model from the notes dataset
notes_final <-
notes %>% select(
noteId,
tweetId,
classification,
trustworthySources,
summary,
isMediaNote
) %>%
mutate(note_length = nchar(summary))
# There is no missing values
vis_miss(slice_sample(notes_final,prop = 0.1))
# the observations in the dataset are unique
length(unique(status$noteId))
# Contains -1 if the note never left “Needs More Ratings” status.
# most of the notes needed more ratings
sum(status$timestampMillisOfFirstNonNMRStatus==-1)
barplot(table(status$currentStatus))
table(status$currentStatus, status$lockedStatus)
# merge data ----
# Notes in both data sets are not exactly the same
# 9.3% of the notes never received ratings
1 - mean(notes_final$noteId %in% rates_summarise$noteId)
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId)) %>%
clean_names() %>% # let's clean the data names
# some notes never received ratings, lte's replace them with 0
>>>>>>> Stashed changes
replace_na(list(ratings = 0,
agreement_rate = 0,
helpful_rate = 0,
not_helpful_rate = 0,
somewhat_helpful_rate = 0))
<<<<<<< Updated upstream
notes_merged <-
left_join(x = notes_merged,
y = status %>%
# select onlye the non duplicated rows
filter(!(note_id %in% duplicated_notes_status)) %>%
# I only analyze the current status
select(note_id,current_status),
by = join_by(note_id))
notes_merged
names(notes_merged)
# however they are all rated as "NEED MORE RATINGS"
duplicated_notes_status <-
status %>% group_by(note_id) %>%
summarise(n_notes = n()) %>%
filter(n_notes>1) %>%
pull(note_id) %>%
format(scientific = F)
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(note_id)) %>%
# some notes never received ratings, let's replace them with 0
replace_na(list(ratings = 0,
agreement_rate = 0,
helpful_rate = 0,
not_helpful_rate = 0,
somewhat_helpful_rate = 0))
notes_merged <-
left_join(x = notes_merged,
y = status %>%
# select onlye the non duplicated rows
filter(!(note_id %in% duplicated_notes_status)) %>%
# I only analyze the current status
select(note_id,current_status),
by = join_by(note_id))
load("data/notes_merged.RData")
# missing data ----
vis_miss(slice_sample(notes_merged,prop = 0.1))
# there are 3 rows with missing values
missing_cell <- which(is.na(notes_merged), arr.ind = TRUE)
notes_merged[missing_cell[,1],]
# These are the tweet ids
notes_merged[missing_cell[,1],] %>% select(tweet_id) %>% pull() %>% format(scientific = F) %>% unique()
=======
notes_merged
save(notes_merged,file = "data/notes_merged.RData")
notes_merged %>% filter(ratings>1000)
notes_merged %>% filter(ratings>1000) %>% arrange(ratings)
notes_merged %>% filter(ratings>1000) %>% arrange(ratings) %>%
select(tweet_id)
notes_merged %>% filter(ratings>8000) %>% arrange(ratings) %>%
select(tweet_id)
notes_merged %>% filter(ratings>8000) %>% arrange(ratings) %>%
select(tweet_id) %>%
pull()
head(notes_merged)
notes_merged %>% filter(ratings>8000) %>% arrange(ratings) %>%
select(tweet_id) %>%
view()
notes_merged %>% filter(ratings>8000) %>% arrange(ratings) %>%
select(tweet_id) %>%
integer()
notes_merged %>% filter(ratings>8000) %>% arrange(ratings) %>%
select(tweet_id) %>%
pull() %>%
integer()
notes_merged %>% filter(ratings>8000) %>% arrange(ratings) %>%
select(tweet_id) %>%
format(scientific = F)
notes_merged %>% filter(ratings>8000) %>% arrange(ratings) %>%
select(tweet_id) %>%
pull() %>%
format(scientific = F)
quantile(notes_merged$ratings,probs = 90)
quantile(notes_merged$ratings,probs = 0.9)
quantile(notes_merged$ratings,probs = 0.99)
q_99 <- quantile(notes_merged$ratings,probs = 0.99)
# 99% of the notes have less than 453 ratings
q_99 <- quantile(notes_merged$ratings,probs = 0.99)
notes_merged %>%
filter(ratings > q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings))
notes_merged %>%
filter(ratings < q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings)) +
labs(
title = "Histogram of the number of Ratings on each Note"
x="Number of Ratings"
notes_merged %>%
filter(ratings < q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings)) +
labs(
title = "Histogram of the number of Ratings on each Note",
x="Number of Ratings"
) +
theme_bw()
notes_merged %>%
filter(ratings >= q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings)) +
labs(
title = "Histogram of the number of Ratings on each Note",
subtitle = "Percentile 99 of the Ratings"
x="Number of Ratings"
notes_merged %>%
filter(ratings >= q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings)) +
labs(
title = "Histogram of the number of Ratings on each Note",
subtitle = "Percentile 99 of the Ratings",
x="Number of Ratings"
) +
theme_bw()
notes_merged %>%
filter(ratings >= q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings)) +
labs(
title = "Histogram of the number of Ratings on each Note",
subtitle = "1% of Notes with more Ratings",
x="Number of Ratings"
) +
theme_bw()
notes_merged %>%
filter(ratings < q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings)) +
labs(
title = "Histogram of the number of Ratings on each Note",
subtitle = "Percentile 99 of the Ratings",
x="Number of Ratings"
) +
theme_bw()
# Correlations ----
# Correlation matrix
notes_cor <- cor(notes_final %>% select_if(is.numeric()))
# Visualization of correlation matrix
notes_corrplot <- corrplot(notes_cor, method = "circle", addCoef.col = 1, number.cex = 0.7)
?corrplot
p_load(tidyverse,
ggplo2,
corrplot)
p_load(tidyverse,
ggplot2,
corrplot)
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor, lower = 'shade', upper = 'pie', order = 'hclust')
# Correlations ----
# Correlation matrix
notes_cor <- cor(notes_final %>% select_if(is.numeric()))
# Correlations ----
# Correlation matrix
notes_cor <- cor(notes_final %>% select_if(is.numeric))
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor, lower = 'shade', upper = 'pie', order = 'hclust')
notes_cor
# Correlations ----
# Correlation matrix
notes_cor <- cor(notes_merged %>% select_if(is.numeric))
notes_merged %>% select_if(is.numeric)
# Correlations ----
# Correlation matrix
notes_cor <- cor(notes_merged %>% select_if(is.numeric))
notes_cor
# there is no missing data in the data set
sum(is.na(notes_merged))
# there is no missing data in the data set
notes_merged[(is.na(notes_merged))]
# there is no missing data in the data set
notes_merged[is.na(notes_merged)]
is.na(notes_merged)
# there is no missing data in the data set
notes_merged[is.na(notes_merged)]
# there is no missing data in the data set
as_data_frame(notes_merged)[is.na(notes_merged)]
# there is no missing data in the data set
as.data.frame(notes_merged)[is.na(notes_merged)]
# there is no missing data in the data set
which(is.na(notes_merged), arr.ind = TRUE)
# there is no missing data in the data set
missing_cell <- which(is.na(notes_merged), arr.ind = TRUE)
notes_merged[missing_cell[,1],]
notes_merged[missing_cell[,1],] %>% View()
notes_merged[missing_cell[,1],] %>% select(tweet_id)
notes_merged[missing_cell[,1],] %>% select(tweet_id) %>% format(scientific = F)
notes_merged[missing_cell[,1],] %>% select(tweet_id) %>% pull() %>% format(scientific = F)
notes_merged[missing_cell[,1],]
# Analyzing the outcome variable ----
summary(notes_merged$ratings)
>>>>>>> Stashed changes
# I remove the missing values, given the content and the number of missing values this shouldn't be an issue
notes_merged <- notes_merged %>% drop_na()
# Analyzing the outcome variable ----
summary(notes_merged$ratings)
# 99% of the notes have less than 453 ratings
q_99 <- quantile(notes_merged$ratings,probs = 0.99)
notes_merged %>%
filter(ratings < q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings)) +
labs(
title = "Histogram of the number of Ratings on each Note",
subtitle = "Percentile 99 of the Ratings",
x="Number of Ratings"
) +
theme_bw()
notes_merged %>%
filter(ratings >= q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings)) +
labs(
title = "Histogram of the number of Ratings on each Note",
subtitle = "1% of Notes with more Ratings",
x="Number of Ratings"
) +
theme_bw()
# The note with most ratings
<<<<<<< Updated upstream
notes_merged %>% filter(ratings>6000) %>% arrange(ratings) %>%
=======
notes_merged %>% filter(ratings>8000) %>% arrange(ratings) %>%
>>>>>>> Stashed changes
select(tweet_id) %>%
pull() %>%
format(scientific = F)
# Correlations ----
# Correlation matrix
<<<<<<< Updated upstream
=======
notes_cor <- cor(notes_merged %>% select_if(is.numeric))
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor, lower = 'shade', upper = 'pie', order = 'hclust')
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor, lower = 'shade', upper = 'pie', order = 'hclust', addCoef.col = 1, number.cex = 0.7)
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor,  addCoef.col = 1, number.cex = 0.7)
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor, method = "circle", lower = 'shade', upper = 'pie', order = 'hclust', addCoef.col = 1, number.cex = 0.7)
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor, method = "circle", addCoef.col = 1, number.cex = 0.7)
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor, lower = 'shade', upper = 'pie', order = 'hclust', addCoef.col = 1, number.cex = 0.7)
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor,
lower = 'shade', upper = 'pie', order = 'hclust',
addCoef.col = 1, number.cex = 0.7,
cl.pos = 'n')
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor,
lower = 'shade', upper = 'pie', order = 'hclust',
addCoef.col = 1, number.cex = 0.7)
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor,
lower = 'shade', upper = 'pie', order = 'hclust',
addCoef.col = 1, number.cex = 0.7,
tl.pos = "ld"
)
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor,
lower = 'shade', upper = 'pie', order = 'hclust',
addCoef.col = 1, number.cex = 0.7,
tl.pos = "lt"
)
# Correlations ----
# Correlation matrix
notes_cor <- cor(notes_merged %>%
select(-c(ends_with("id")))
select_if(is.numeric))
# Correlations ----
# Correlation matrix
notes_cor <- cor(notes_merged %>%
select(ends_with("id")) %>%
select_if(is.numeric))
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor,
lower = 'shade', upper = 'pie', order = 'hclust',
addCoef.col = 1, number.cex = 0.7,
tl.pos = "lt"
)
# Correlations ----
# Correlation matrix
>>>>>>> Stashed changes
notes_cor <- cor(notes_merged %>%
select(-ends_with("id")) %>%
select_if(is.numeric))
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor,
lower = 'shade', upper = 'pie', order = 'hclust',
addCoef.col = 1, number.cex = 0.7,
tl.pos = "lt"
)
<<<<<<< Updated upstream
p_load(tidyverse,
ggplot2,
corrplot)
# Correlations ----
# Correlation matrix
notes_cor <- cor(notes_merged %>%
select(-ends_with("id")) %>%
select_if(is.numeric))
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor,
lower = 'shade', upper = 'pie', order = 'hclust',
addCoef.col = 1, number.cex = 0.7,
tl.pos = "lt"
)
notes_merged %>%
filter(ratings > 1, ratings < 2000) %>%
ggplot() +
geom_boxplot(aes(x=current_status, y = ratings))
notes_merged %>% group_by(current_status) %>%
summarise(mean(ratings), median(ratings),
quantile(ratings,probs = 0.1),
quantile(ratings,probs = 0.9)
)
table(notes_merged$classification, notes_merged$current_status)
# Analyzing the outcome variable ----
summary(notes_merged$ratings)
# 99% of the notes have less than 453 ratings
q_99 <- quantile(notes_merged$ratings,probs = 0.99)
notes_merged %>%
filter(ratings < q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings)) +
labs(
title = "Histogram of the number of Ratings on each Note",
subtitle = "Percentile 99 of the Ratings",
x="Number of Ratings"
) +
theme_bw()
notes_merged %>%
filter(ratings >= q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings)) +
labs(
title = "Histogram of the number of Ratings on each Note",
subtitle = "1% of Notes with more Ratings",
x="Number of Ratings"
) +
theme_bw()
# The note with most ratings
notes_merged %>% filter(ratings>6000) %>% arrange(ratings) %>%
select(tweet_id) %>%
pull() %>%
format(scientific = F)
notes_merged %>%
filter(ratings > 1, ratings < 2000) %>%
ggplot() +
geom_boxplot(aes(x=current_status, y = ratings))
notes_merged %>% group_by(current_status) %>%
summarise(mean(ratings), median(ratings),
quantile(ratings,probs = 0.1),
quantile(ratings,probs = 0.9)
)
table(notes_merged$classification, notes_merged$current_status)
knitr::opts_chunk$set(echo = TRUE)
notes_merged %>% group_by(classification) %>%
summarise(mean(ratings), median(ratings),
quantile(ratings,probs = 0.1),
quantile(ratings,probs = 0.9)
)
=======
>>>>>>> Stashed changes
