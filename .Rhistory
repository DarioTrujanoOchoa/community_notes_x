mean(r3$noteId %in% r1$noteId)
# Most notes have few ratings
barplot(table(table(r0$noteId)),
xlab = "Number of Ratings",
ylab = "Number of Notes",
main = "Distribution of Ratings Published by Note")
table(r0$helpfulnessLevel)
rates_summarise <-
bind_rows(r0,
r1,
r2,
r3) %>%
group_by(noteId) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful = sum(helpfulnessLevel =="HELPFUL",na.rm = T),
not_helpful = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)
)
rates_summarise
# merge data ----
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId))
notes_merged
save(notes_merged,file = "data/notes_merged.RData")
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes,
rsample,
parsnip,
workflows
)
load("data/notes_merged.RData")
vis_miss(slice_sample(notes_merged,prop = 0.1))
set.seed(1984)
training_percentage <- 0.75
split_notes <- initial_split(notes_merged,prop = training_percentage)
training_notes <- training(split_notes)
test_notes <- testing(split_notes)
# recipe ----
rec_reg <- recipe(ratings ~  .,
data = training_notes %>% select(-c(noteId,summary))) %>%
step_normalize(agreement_rate)
# models ----
##linear model ----
linear_reg <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
# KNN model
k = 7
knn_mod <- nearest_neighbor(neighbors = k) %>%
set_mode("regression") %>%
set_engine("kknn")
# Workflow ----
## linear ----
lm_wkflow <- workflow() %>%
# add model
add_model(linear_reg) %>%
# add receipe
add_recipe(rec_reg)
## KNNn model ----
knn_wkflow <- workflow() %>%
# add model
add_model(knn_mod) %>%
# add receipe
add_recipe(rec_reg)
# fitting models ----
## linear ----
fit_lm <-
lm_wkflow %>%
fit(data = training_notes)
fit_lm
## KNN ----
fit_knn <-
knn_wkflow %>%
fit(data = training_notes)
fit_knn
# metrics
notes_metrics <- metric_set(rmse, rsq, mae)
# linear model
notes_lm_aug <- augment(fit_lm, test_notes)
notes_metrics(notes_lm_aug, truth = ratings,
estimate = .pred)
notes_merged %>% ggplot() +
geom_point(aes(x=agreement_rate,y=ratings))
# knn model
notes_knn_aug <- augment(fit_knn, test_notes)
notes_metrics(notes_knn_aug, truth = ratings,
estimate = .pred)
View(notes_merged)
load("data/notes_merged.RData")
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary))
# packages
library(pacman)
p_load(tidyverse,
lubridate,
naniar,
janitor,
forcats)
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary))
# load data ----
# all the data was downloaded on December 3rd 2023
# notes
notes <- read_tsv("data/notes-00000.tsv") %>% clean_names()
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary))
# merge data ----
# Notes in both data sets are not exactly the same
# 9.3% of the notes never received ratings
1 - mean(notes_final$note_id %in% rates_summarise$note_id)
notes_final$note_id
# merge data ----
# Notes in both data sets are not exactly the same
# 9.3% of the notes never received ratings
1 - mean(notes_final$note_id %in% rates_summarise$note_id)
r1 <- read_tsv("data/ratings-00001.tsv") %>% clean_names()
names(notes_merged)
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary)) %>%
select(-created_at_millis)
# There is no missing values
vis_miss(slice_sample(notes_final,prop = 0.1))
## status
status <- read_tsv("data/noteStatusHistory-00000.tsv") %>% clean_names()
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary)) %>%
select(-created_at_millis)
## select variables from ratings at the notes level ----
rates_summarise <-
bind_rows(r0,
r1,
r2,
r3) %>%
group_by(note_id) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulness_level =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulness_level =="NOT_HELPFUL",na.rm = T)/n(),
somewhat_helpful_rate = sum(helpfulness_level =="SOMEWHAT_HELPFUL",na.rm = T)/n()
)
# merge data ----
# Notes in both data sets are not exactly the same
# 9.3% of the notes never received ratings
1 - mean(notes_final$note_id %in% rates_summarise$note_id)
rates_summarise
## select variables from ratings at the notes level ----
rates_summarise <-
bind_rows(r0,
r1,
r2,
r3) %>%
group_by(note_id) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulness_level =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulness_level =="NOT_HELPFUL",na.rm = T)/n(),
somewhat_helpful_rate = sum(helpfulness_level =="SOMEWHAT_HELPFUL",na.rm = T)/n()
)
## select variables from ratings at the notes level ----
rates_summarise <-
bind_rows(r0,
r1,
#r2,
r3) %>%
group_by(note_id) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulness_level =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulness_level =="NOT_HELPFUL",na.rm = T)/n(),
somewhat_helpful_rate = sum(helpfulness_level =="SOMEWHAT_HELPFUL",na.rm = T)/n()
)
# merge data ----
# Notes in both data sets are not exactly the same
# 9.3% of the notes never received ratings
1 - mean(notes_final$note_id %in% rates_summarise$note_id)
## select variables from ratings at the notes level ----
rates_summarise <-
bind_rows(r0,
#r1,
r2,
r3) %>%
group_by(note_id) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulness_level =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulness_level =="NOT_HELPFUL",na.rm = T)/n(),
somewhat_helpful_rate = sum(helpfulness_level =="SOMEWHAT_HELPFUL",na.rm = T)/n()
)
# merge data ----
# Notes in both data sets are not exactly the same
# 9.3% of the notes never received ratings
1 - mean(notes_final$note_id %in% rates_summarise$note_id)
r1 <- read_tsv("data/ratings-00001.tsv") %>% clean_names()
r2 <- read_tsv("data/ratings-00002.tsv") %>% clean_names()
r3 <- read_tsv("data/ratings-00003.tsv") %>% clean_names()
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary)) %>%
select(-c(created_at_millis,
summary))
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary)) %>%
select(-c(created_at_millis,
summary))
names(notes_final)
## Select the variables that will be used in the model from the notes dataset ----
notes_final <-
notes %>% select(
note_id,
tweet_id,
classification,
trustworthy_sources,
summary,
is_media_note,
created_at_millis
) %>%
mutate(created_at = as.POSIXct(created_at_millis, origin="1970-01-01")) %>%
mutate(w_day = wday(created_at, label = T),
hour = as_factor(hour(created_at)),
note_length = nchar(summary)) %>%
select(-c(created_at_millis,
summary))
## select variables from ratings at the notes level ----
rates_summarise <-
bind_rows(r0,
r1,
r2,
r3) %>%
group_by(note_id) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulness_level =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulness_level =="NOT_HELPFUL",na.rm = T)/n(),
somewhat_helpful_rate = sum(helpfulness_level =="SOMEWHAT_HELPFUL",na.rm = T)/n()
)
## select variables from ratings at the notes level ----
rates_summarise <-
bind_rows(r0,
#r1,
r2,
r3) %>%
group_by(note_id) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful_rate = sum(helpfulness_level =="HELPFUL",na.rm = T)/n(),
not_helpful_rate = sum(helpfulness_level =="NOT_HELPFUL",na.rm = T)/n(),
somewhat_helpful_rate = sum(helpfulness_level =="SOMEWHAT_HELPFUL",na.rm = T)/n()
)
# merge data ----
# Notes in both data sets are not exactly the same
# 9.3% of the notes never received ratings
1 - mean(notes_final$note_id %in% rates_summarise$note_id)
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(note_id)) %>%
# some notes never received ratings, let's replace them with 0
replace_na(list(ratings = 0,
agreement_rate = 0,
helpful_rate = 0,
not_helpful_rate = 0,
somewhat_helpful_rate = 0))
notes_merged <-
left_join(x = notes_merged,
y = status %>%
# select onlye the non duplicated rows
filter(!(note_id %in% duplicated_notes_status)) %>%
# I only analyze the current status
select(note_id,current_status),
by = join_by(note_id))
notes_merged
names(notes_merged)
# however they are all rated as "NEED MORE RATINGS"
duplicated_notes_status <-
status %>% group_by(note_id) %>%
summarise(n_notes = n()) %>%
filter(n_notes>1) %>%
pull(note_id) %>%
format(scientific = F)
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(note_id)) %>%
# some notes never received ratings, let's replace them with 0
replace_na(list(ratings = 0,
agreement_rate = 0,
helpful_rate = 0,
not_helpful_rate = 0,
somewhat_helpful_rate = 0))
notes_merged <-
left_join(x = notes_merged,
y = status %>%
# select onlye the non duplicated rows
filter(!(note_id %in% duplicated_notes_status)) %>%
# I only analyze the current status
select(note_id,current_status),
by = join_by(note_id))
load("data/notes_merged.RData")
# missing data ----
vis_miss(slice_sample(notes_merged,prop = 0.1))
# there are 3 rows with missing values
missing_cell <- which(is.na(notes_merged), arr.ind = TRUE)
notes_merged[missing_cell[,1],]
# These are the tweet ids
notes_merged[missing_cell[,1],] %>% select(tweet_id) %>% pull() %>% format(scientific = F) %>% unique()
# I remove the missing values, given the content and the number of missing values this shouldn't be an issue
notes_merged <- notes_merged %>% drop_na()
# Analyzing the outcome variable ----
summary(notes_merged$ratings)
# 99% of the notes have less than 453 ratings
q_99 <- quantile(notes_merged$ratings,probs = 0.99)
notes_merged %>%
filter(ratings < q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings)) +
labs(
title = "Histogram of the number of Ratings on each Note",
subtitle = "Percentile 99 of the Ratings",
x="Number of Ratings"
) +
theme_bw()
notes_merged %>%
filter(ratings >= q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings)) +
labs(
title = "Histogram of the number of Ratings on each Note",
subtitle = "1% of Notes with more Ratings",
x="Number of Ratings"
) +
theme_bw()
# The note with most ratings
notes_merged %>% filter(ratings>6000) %>% arrange(ratings) %>%
select(tweet_id) %>%
pull() %>%
format(scientific = F)
# Correlations ----
# Correlation matrix
notes_cor <- cor(notes_merged %>%
select(-ends_with("id")) %>%
select_if(is.numeric))
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor,
lower = 'shade', upper = 'pie', order = 'hclust',
addCoef.col = 1, number.cex = 0.7,
tl.pos = "lt"
)
p_load(tidyverse,
ggplot2,
corrplot)
# Correlations ----
# Correlation matrix
notes_cor <- cor(notes_merged %>%
select(-ends_with("id")) %>%
select_if(is.numeric))
# Visualization of correlation matrix
notes_corrplot <- corrplot.mixed(notes_cor,
lower = 'shade', upper = 'pie', order = 'hclust',
addCoef.col = 1, number.cex = 0.7,
tl.pos = "lt"
)
notes_merged %>%
filter(ratings > 1, ratings < 2000) %>%
ggplot() +
geom_boxplot(aes(x=current_status, y = ratings))
notes_merged %>% group_by(current_status) %>%
summarise(mean(ratings), median(ratings),
quantile(ratings,probs = 0.1),
quantile(ratings,probs = 0.9)
)
table(notes_merged$classification, notes_merged$current_status)
# Analyzing the outcome variable ----
summary(notes_merged$ratings)
# 99% of the notes have less than 453 ratings
q_99 <- quantile(notes_merged$ratings,probs = 0.99)
notes_merged %>%
filter(ratings < q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings)) +
labs(
title = "Histogram of the number of Ratings on each Note",
subtitle = "Percentile 99 of the Ratings",
x="Number of Ratings"
) +
theme_bw()
notes_merged %>%
filter(ratings >= q_99) %>%
ggplot() +
geom_histogram(aes(x= ratings)) +
labs(
title = "Histogram of the number of Ratings on each Note",
subtitle = "1% of Notes with more Ratings",
x="Number of Ratings"
) +
theme_bw()
# The note with most ratings
notes_merged %>% filter(ratings>6000) %>% arrange(ratings) %>%
select(tweet_id) %>%
pull() %>%
format(scientific = F)
notes_merged %>%
filter(ratings > 1, ratings < 2000) %>%
ggplot() +
geom_boxplot(aes(x=current_status, y = ratings))
notes_merged %>% group_by(current_status) %>%
summarise(mean(ratings), median(ratings),
quantile(ratings,probs = 0.1),
quantile(ratings,probs = 0.9)
)
table(notes_merged$classification, notes_merged$current_status)
knitr::opts_chunk$set(echo = TRUE)
notes_merged %>% group_by(classification) %>%
summarise(mean(ratings), median(ratings),
quantile(ratings,probs = 0.1),
quantile(ratings,probs = 0.9)
)
