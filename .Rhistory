select_if(is.numeric) %>%
map_dbl(sum) %>% as.data.frame()
vis_miss(s_notes)
status <- read_tsv("data/noteStatusHistory-00000.tsv")
spec(status)
s_status <- slice_sample(status,n = 10000)
vis_miss(s_status)
# the observations in the dataset are unique
length(unique(s_status$noteId))
sum(s_notes$noteId %in% s_status$noteId)
sum(notes$noteId %in% status$noteId)
barplot(status$timestampMillisOfFirstNonNMRStatus)
barplot(s_status$timestampMillisOfFirstNonNMRStatus)
s_status$timestampMillisOfFirstNonNMRStatus
sum(s_status$timestampMillisOfFirstNonNMRStatus==-1)
length(unique(s_notes$tweetId))
length(unique(s_notes$tweetId))
# notes ----
# the observations in the dataset are unique for each note
length(unique(notes$noteId))
length(unique(notes$tweetId))
barplot(table(table(s_notes$tweetId)),
xlab = "Number of Notes",
ylab = "Number of Authors",
main = "Distribution of Notes Published by Author")
table(table(s_notes$tweetId)
)
barplot(table(table(s_notes$tweetId)),
xlab = "Number of Notes",
ylab = "Number of Tweets",
main = "Distribution of Notes Published by Author")
barplot(table(table(s_notes$tweetId)),
xlab = "Number of Notes",
ylab = "Number of Tweets",
main = "Distribution of Notes Published by Tweet")
## Tweets ----
# most of the notes refer to a single
length(unique(notes$tweetId))
## Tweets ----
# some tweets have more than one note
# most of the tweets have a single note
length(unique(notes$tweetId))
barplot(table(table(s_notes$tweetId)),
xlab = "Number of Notes",
ylab = "Number of Tweets",
main = "Distribution of Notes Published by Tweet")
table(table(s_notes$tweetId)
)
as_tibble(s_notes) %>%
filter(classification == "MISINFORMED_OR_POTENTIALLY_MISLEADING") %>%
select_if(is.numeric) %>%
map_dbl(sum) %>% as.data.frame()
table(s_notes$classification,s_notes$notMisleadingOther)
table(s_notes$classification,s_notes$isMediaNote)
table(s_notes$classification,s_notes$trustworthySources)
# Contains -1 if the note never left “Needs More Ratings” status.
sum(s_status$timestampMillisOfFirstNonNMRStatus==-1)
# Contains -1 if the note never left “Needs More Ratings” status.
sum(status$timestampMillisOfFirstNonNMRStatus==-1)
barplot(status$currentStatus)
status$currentStatus
barplot(table(status$currentStatus))
# the observations in the dataset are unique
length(unique(s_status$noteId))
# the observations in the dataset are unique
length(unique(status$noteId))
table(status$currentStatus, status$lockedStatus)
r0 <- read_tsv("data/ratings-00000.tsv")
r1 <- read_tsv("data/ratings-00001.tsv")
r0
r0 <- read_tsv("data/ratings-00000.tsv")
spec(r0)
sum(notes$noteId %in% r0$noteId)
# most of the notes are in the ratings dataset
sum(r0$noteId %in% notes$noteId)
# most of the notes are in the ratings dataset
sum(r1$noteId %in% notes$noteId)
barplot(table(table(r0$noteId)),
xlab = "Number of Notes",
ylab = "Number of Tweets",
main = "Distribution of Notes Published by Tweet")
barplot(table(table(r0$noteId)),
xlab = "Number of Ratings",
ylab = "Number of Notes",
main = "Distribution of Notes Published by Tweet")
table(table(r0$noteId)
)
library(pacman)
p_load(tidyverse,
naniar)
# load data ----
notes <- read_tsv("data/notes-00000.tsv")
spec(notes)
status <- read_tsv("data/noteStatusHistory-00000.tsv")
spec(status)
r0 <- read_tsv("data/ratings-00000.tsv")
spec(r0)
# most of the notes are in the ratings dataset
sum(r0$noteId %in% notes$noteId)
sum(notes$noteId %in% r0$noteId)
barplot(table(table(r0$noteId)),
xlab = "Number of Ratings",
ylab = "Number of Notes",
main = "Distribution of Ratings Published by Note")
table(status$currentStatus, status$lockedStatus)
table(status$currentStatus, status$lockedStatus)
length(unique(r0$raterParticipantId))
barplot(table(table(r0$raterParticipantId)),
xlab = "Number of Ratings",
ylab = "Number of Raters",
main = "Distribution of Ratings Published by Rater")
r0 %>% group_by(noteId) %>% summarise(ratings = n(),agreements = sum(agree))
table(r0$helpfulnessLevel)
r0 %>%
group_by(noteId) %>%
summarise(ratings = n(),
agreements = sum(agree),
helpful = sum(helpfulnessLevel =="HELPFUL"))
r0 %>%
group_by(noteId) %>%
summarise(ratings = n(),
agreements = sum(agree),
helpful = sum(helpfulnessLevel =="HELPFUL",na.rm = T))
r0 %>%
group_by(noteId) %>%
summarise(ratings = n(),
agreements = sum(agree),
helpful = sum(helpfulnessLevel =="HELPFUL",na.rm = T),
not_helpful = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)
)
rates_summarise <-
r0 %>%
group_by(noteId) %>%
summarise(ratings = n(),
agreements = sum(agree),
helpful = sum(helpfulnessLevel =="HELPFUL",na.rm = T),
not_helpful = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)
)
s_notes <- slice_sample(notes,n = 10000)
s_status <- slice_sample(status,n = 10000)
vis_miss(s_notes)
vis_miss(s_status)
s_notes$summary
s_notes$summary %>% nchar()
notes %>% select(classification,
trustworthySources,
summary,
isMediaNote
) %>%
mutate(note_length = nchar(summary))
rates_summarise
notes_final %>% select(classification,
trustworthySources,
summary,
isMediaNote
) %>%
mutate(note_length = nchar(summary))
notes_final <-
note %>% select(classification,
trustworthySources,
summary,
isMediaNote
) %>%
mutate(note_length = nchar(summary))
notes_final <-
notes %>% select(classification,
trustworthySources,
summary,
isMediaNote
) %>%
mutate(note_length = nchar(summary))
notes_merged <- left_join(rates_summarise,notes_final)
notes_merged <- left_join(notes_final, rates_summarise)
notes_final <-
notes %>% select(
noteId,
classification,
trustworthySources,
summary,
isMediaNote
) %>%
mutate(note_length = nchar(summary))
rates_summarise <-
r0 %>%
group_by(noteId) %>%
summarise(noteId = noteId,
ratings = n(),
agreements = sum(agree),
helpful = sum(helpfulnessLevel =="HELPFUL",na.rm = T),
not_helpful = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)
)
rates_summarise
rates_summarise <-
r0 %>%
group_by(noteId) %>%
summarise(
ratings = n(),
agreements = sum(agree),
helpful = sum(helpfulnessLevel =="HELPFUL",na.rm = T),
not_helpful = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)
)
rates_summarise
notes_merged <- left_join(notes_final, rates_summarise)
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId))
notes_merged
lm(ratings ~ classification ,data = notes_merged)
lm(ratings ~
classification +
trustworthySources +
note_length,
data = notes_merged)
rates_summarise <-
r0 %>%
group_by(noteId) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful = sum(helpfulnessLevel =="HELPFUL",na.rm = T),
not_helpful = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)
)
split_notes <- initial_split(notes_merged,prop = training_percentage)
library(pacman)
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune)
p_load(ggplot2)
p_load(ggthemes)
set.seed(1984)
training_percentage <- 0.75
split_notes <- initial_split(notes_merged,prop = training_percentage)
training_note <- training(split_note)
training_notes <- training(split_notes)
test_notes <- testing(split_notes)
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes,
rsample,
parsnip,
workflows
)
vis_miss(slice_sample(notes_merged,prop = 0.1))
set.seed(1984)
training_percentage <- 0.75
split_notes <- initial_split(notes_merged,prop = training_percentage)
training_notes <- training(split_notes)
test_notes <- testing(split_notes)
# recipe ----
rec_reg <- recipe(ratings ~  .,
data = training_notes %>% select(-c(noteId,summary))) %>%
step_normalize(agreement_rate)
# models ----
##linear model ----
linear_reg <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
# KNN model
k = 7
knn_mod <- nearest_neighbor(neighbors = k) %>%
set_mode("regression") %>%
set_engine("kknn")
# Workflow ----
## linear ----
lm_wkflow <- workflow() %>%
# add model
add_model(linear_reg) %>%
# add receipe
add_recipe(rec_reg)
## KNNn model ----
knn_wkflow <- workflow() %>%
# add model
add_model(knn_mod) %>%
# add receipe
add_recipe(rec_reg)
# fitting models ----
## linear ----
fit_lm <-
lm_wkflow %>%
fit(data = training_notes)
fit_lm
# fitting models ----
## linear ----
fit_lm <-
lm_wkflow %>%
fit(data = training_notes)
p_load(tidyverse,
lubridate,
naniar)
# load data ----
# notes
notes <- read_tsv("data/notes-00000.tsv")
spec(notes)
spec(notes)
p_load(tidyverse,
lubridate,
naniar)
# load data ----
# notes
notes <- read_tsv("data/notes-00000.tsv")
spec(notes)
spec(status)
## status
status <- read_tsv("data/noteStatusHistory-00000.tsv")
spec(status)
## ratings
r0 <- read_tsv("data/ratings-00000.tsv")
spec(r0)
# Almost all the notes in notes are in status
# only 20 notes are not there
sum(notes$noteId %in% status$noteId)
s_notes <- slice_sample(notes,n = 10000)
s_status <- slice_sample(status,n = 10000)
# The variables Believable, harmful, and validation difficulty were Deprecated as of 2022-10-27.
vis_miss(s_notes)
<<<<<<< Updated upstream
r2 <- read_tsv("data/ratings-00002.tsv")
r3 <- read_tsv("data/ratings-00003.tsv")
library(pacman)
p_load(tidyverse,
lubridate,
naniar)
r2 <- read_tsv("data/ratings-00002.tsv")
r3 <- read_tsv("data/ratings-00003.tsv")
s_r0 <- slice_sample(prop = 0.01)
s_r0 <- r0 %>% slice_sample(prop = 0.01)
s_r2 <- r2 %>% slice_sample(prop = 0.01)
s_r3 <- r3 %>% slice_sample(prop = 0.01)
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId))
bind_cols(s_r0,s_r2)
rlang::last_trace()
rlang::last_trace(drop = FALSE)
bind_rows(s_r0,s_r2)
rates_summarise <-
bind_rows(r0,
#r1,
=======
vis_miss(s_status)
## Authors ----
# the observations in the dataset are unique for each note
length(unique(notes$noteId))
# the Some notes are made by more than one participant
length(unique(s_notes$noteAuthorParticipantId))
# most users publish a note only once (60% in sample)
table(table(s_notes$noteAuthorParticipantId))
barplot(table(table(s_notes$noteAuthorParticipantId)),
xlab = "Number of Notes",
ylab = "Number of Authors",
main = "Distribution of Notes Published by Author")
# ID of prolific authors
head(sort(table(s_notes$noteAuthorParticipantId),decreasing = T),n = 25 )
## Tweets ----
# some tweets have more than one note
# most of the tweets have a single note
length(unique(notes$tweetId))
barplot(table(table(s_notes$tweetId)),
xlab = "Number of Notes",
ylab = "Number of Tweets",
main = "Distribution of Notes Published by Tweet")
as_tibble(s_notes) %>%
filter(classification == "MISINFORMED_OR_POTENTIALLY_MISLEADING") %>%
select_if(is.numeric) %>%
map_dbl(sum) %>% as.data.frame()
table(s_notes$classification,s_notes$notMisleadingOther)
table(s_notes$classification,s_notes$isMediaNote)
table(s_notes$classification,s_notes$trustworthySources)
# select the variables that will be used in the model from the notes dataset
notes_final <-
notes %>% select(
noteId,
classification,
trustworthySources,
summary,
isMediaNote
) %>%
mutate(note_length = nchar(summary))
# the observations in the dataset are unique
length(unique(status$noteId))
# Contains -1 if the note never left “Needs More Ratings” status.
# most of the notes needed more ratings
sum(status$timestampMillisOfFirstNonNMRStatus==-1)
barplot(table(status$currentStatus))
table(status$currentStatus, status$lockedStatus)
# there are 109,142 raters. Small for the number of notes.
# they cover
length(unique(r0$raterParticipantId))
# only 13 people in r3 were in r1
# 42 people in r1 were in r3
# There is no more repetitions
sum(r1$raterParticipantId %in%
r3$raterParticipantId)
barplot(table(table(r0$raterParticipantId)),
xlab = "Number of Ratings",
ylab = "Number of Raters",
main = "Distribution of Ratings Published by Rater")
## ratings in notes ----
# most of the notes are in the ratings dataset
sum(r0$noteId %in% notes$noteId)
sum(r1$noteId %in% notes$noteId)
sum(r2$noteId %in% notes$noteId)
sum(r3$noteId %in% notes$noteId)
mean(notes$noteId %in% r0$noteId)
mean(notes$noteId %in% r1$noteId)
mean(notes$noteId %in% r2$noteId)
mean(notes$noteId %in% r3$noteId)
mean(r3$noteId %in% r1$noteId)
# Most notes have few ratings
barplot(table(table(r0$noteId)),
xlab = "Number of Ratings",
ylab = "Number of Notes",
main = "Distribution of Ratings Published by Note")
table(r0$helpfulnessLevel)
r2 <- read_tsv("data/ratings-00002.tsv")
r3 <- read_tsv("data/ratings-00003.tsv")
rates_summarise <-
bind_rows(r0,
r1,
>>>>>>> Stashed changes
r2,
r3) %>%
group_by(noteId) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful = sum(helpfulnessLevel =="HELPFUL",na.rm = T),
not_helpful = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)
)
<<<<<<< Updated upstream
r1 <- read_tsv("data/ratings-00001.tsv")
s_r1 <- r1 %>% slice_sample(prop = 0.01)
rates_summarise <-
bind_rows(s_r0,
s_r1,
s_r2,
s_r3) %>%
group_by(noteId) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful = sum(helpfulnessLevel =="HELPFUL",na.rm = T),
not_helpful = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)
)
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId))
# select the variables that will be used in the model from the notes dataset
notes_final <-
notes %>% select(
noteId,
classification,
trustworthySources,
summary,
isMediaNote
) %>%
mutate(note_length = nchar(summary))
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId))
rates_summarise <-
bind_rows(r0,
r1,
=======
rates_summarise <-
bind_rows(r0,
# r1,
>>>>>>> Stashed changes
r2,
r3) %>%
group_by(noteId) %>%
summarise(
ratings = n(),
agreement_rate = sum(agree)/n(),
helpful = sum(helpfulnessLevel =="HELPFUL",na.rm = T),
not_helpful = sum(helpfulnessLevel =="NOT_HELPFUL",na.rm = T)
)
<<<<<<< Updated upstream
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId))
notes_merged
save(notes_merged,file = "data/notes_merged.RData")
load("data/notes_merged.RData")
vis_miss(s_notes)
vis_miss(s_notes==0)
# The variables Believable, harmful, and validation difficulty were Deprecated as of 2022-10-27.
vis_miss(notes_merged)
# The variables Believable, harmful, and validation difficulty were Deprecated as of 2022-10-27.
vis_miss(slice_sample(notes_merged,prop = 0.1))
set.seed(1984)
training_percentage <- 0.75
split_notes <- initial_split(notes_merged,prop = training_percentage)
training_notes <- training(split_notes)
test_notes <- testing(split_notes)
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune)
p_load(ggplot2)
p_load(ggthemes)
split_notes <- initial_split(notes_merged,prop = training_percentage)
training_notes <- training(split_notes)
test_notes <- testing(split_notes)
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes)
split_notes <- initial_split(notes_merged,prop = training_percentage)
?initial_split
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes)
library(tidymodels)
pacman::p_load(ggplot2)
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes)
library(pacman)
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune)
p_load(ggplot2)
p_load(ggthemes)
set.seed(1984)
training_percentage <- 0.75
split_abalone <- initial_split(abalone,prop = training_percentage, strata = type)
training_abalone <- training(split_abalone)
test_abalone <- testing(split_abalone)
library(tidymodels)
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
# ggplot2,
ggthemes)
=======
rates_summarise
notes_merged <- left_join(notes_final, rates_summarise, by = join_by(noteId))
notes_merged
save(notes_merged,file = "data/notes_merged.RData")
lm(ratings ~
classification +
trustworthySources +
note_length,
data = notes_merged)
p_load(tidyverse,
lubridate,
naniar)
>>>>>>> Stashed changes
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes,
<<<<<<< Updated upstream
rsample)
split_notes <- initial_split(notes_merged,prop = training_percentage)
training_notes <- training(split_notes)
test_notes <- testing(split_notes)
=======
rsample,
parsnip,
workflows
)
load("data/notes_merged.RData")
>>>>>>> Stashed changes
vis_miss(slice_sample(notes_merged,prop = 0.1))
set.seed(1984)
training_percentage <- 0.75
split_notes <- initial_split(notes_merged,prop = training_percentage)
training_notes <- training(split_notes)
test_notes <- testing(split_notes)
<<<<<<< Updated upstream
# recipe
rec_reg <- recipe(ratings ~  .,
data = notes_merged %>% select(-c(noteId,summary)))
# linear model
linear_reg <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
?set_engine
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes,
rsample,
parsnip
)
# linear model
linear_reg <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
# Workflow ----
## empty workflow
lm_wkflow <- workflow() %>%
# add model
add_model(linear_reg) %>%
# add receipe
add_recipe(rec_reg)
?add_recipe
p_load(tidyverse,
tidymodels,
recipes,
kknn,
yardstick,
tune,
ggplot2,
ggthemes,
rsample,
parsnip,
workflows
)
# Workflow ----
## empty workflow
lm_wkflow <- workflow() %>%
# add model
add_model(linear_reg) %>%
# add receipe
add_recipe(rec_reg)
# fitting models
fit_lm <-
lm_wkflow %>%
fit(data = training_abalone)
# recipe ----
rec_reg <- recipe(ratings ~  .,
data = training_notes %>% select(-c(noteId,summary)))
# linear model ----
linear_reg <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
# Workflow ----
## empty workflow
lm_wkflow <- workflow() %>%
# add model
add_model(linear_reg) %>%
# add receipe
add_recipe(rec_reg)
# fitting models
fit_lm <-
lm_wkflow %>%
fit(data = training_notes)
fit_lm
# metrics
notes_metrics <- metric_set(rmse, rsq, mae)
# linear model
notes_lm_aug <- augment(fit_lm,test_abalone)
# linear model
notes_lm_aug <- augment(fit_lm, test_notes)
notes_metrics(notes_lm_aug, truth = age,
estimate = .pred)
# linear model
notes_lm_aug <- augment(fit_lm, test_notes)
notes_metrics(notes_lm_aug, truth = ratings,
estimate = .pred)
fit_lm
=======
>>>>>>> Stashed changes
# recipe ----
rec_reg <- recipe(ratings ~  .,
data = training_notes %>% select(-c(noteId,summary))) %>%
step_normalize(agreement_rate)
<<<<<<< Updated upstream
# linear model ----
linear_reg <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
# Workflow ----
## empty workflow
lm_wkflow <- workflow() %>%
# add model
add_model(linear_reg) %>%
# add receipe
add_recipe(rec_reg)
# fitting models
fit_lm <-
lm_wkflow %>%
fit(data = training_notes)
fit_lm
# metrics
notes_metrics <- metric_set(rmse, rsq, mae)
# linear model
notes_lm_aug <- augment(fit_lm, test_notes)
notes_metrics(notes_lm_aug, truth = ratings,
estimate = .pred)
?metric_set
notes_merged %>% ggplot() %>%
notes_merged %>% ggplot() +
geom_point(aes(x=agreement_rate,y=ratings))
=======
# models ----
##linear model ----
linear_reg <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
# KNN model
k = 7
>>>>>>> Stashed changes
knn_mod <- nearest_neighbor(neighbors = k) %>%
set_mode("regression") %>%
set_engine("kknn")
# Workflow ----
## linear ----
lm_wkflow <- workflow() %>%
# add model
add_model(linear_reg) %>%
# add receipe
add_recipe(rec_reg)
## KNNn model ----
knn_wkflow <- workflow() %>%
# add model
add_model(knn_mod) %>%
# add receipe
add_recipe(rec_reg)
<<<<<<< Updated upstream
## KNN ----
fit_knn <-
knn_wkflow %>%
fit(data = training_abalone)
## KNN ----
fit_knn <-
knn_wkflow %>%
fit(data = training_notes)
=======
# fitting models ----
## linear ----
fit_lm <-
lm_wkflow %>%
fit(data = training_notes)
fit_lm
>>>>>>> Stashed changes
## KNN ----
fit_knn <-
knn_wkflow %>%
fit(data = training_notes)
<<<<<<< Updated upstream
# KNN model
k = 7
knn_mod <- nearest_neighbor(neighbors = k) %>%
set_mode("regression") %>%
set_engine("kknn")
fit_knn
## KNN ----
fit_knn <-
knn_wkflow %>%
fit(data = training_notes)
=======
fit_knn
# metrics
notes_metrics <- metric_set(rmse, rsq, mae)
# linear model
notes_lm_aug <- augment(fit_lm, test_notes)
notes_metrics(notes_lm_aug, truth = ratings,
estimate = .pred)
notes_merged %>% ggplot() +
geom_point(aes(x=agreement_rate,y=ratings))
# linear model
notes_knn_aug <- augment(fit_knn, test_notes)
notes_metrics(notes_knn_aug, truth = ratings,
estimate = .pred)
fit_knn
# linear model
notes_lm_aug <- augment(fit_lm, test_notes)
fit_knn
# knn model
notes_knn_aug <- augment(fit_knn, test_notes)
notes_knn_aug
notes_metrics(notes_lm_aug, truth = ratings,
estimate = .pred)
>>>>>>> Stashed changes
